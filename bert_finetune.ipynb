{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "eb3ae4371d214130b82ecd897e5c918d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e713a4ba99d436d9efb0011bb74bf8c",
       "IPY_MODEL_5b69a6e548724954855ef5cefe8cbd45",
       "IPY_MODEL_a8f02048df2b41c396ad60400fde9134"
      ],
      "layout": "IPY_MODEL_cc26941f2a4f455d9e3fff49a0d690f7"
     }
    },
    "1e713a4ba99d436d9efb0011bb74bf8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_593fbc1b8eba49b7857d0d3a73329eb5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d8b4aeb193348a5891137a7c6e7df53",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "5b69a6e548724954855ef5cefe8cbd45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e63aa202f7954802a6bdb3b87a177b01",
      "max": 498818054,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d8858c31d964ca085a73b1c06b2801c",
      "value": 498818054
     }
    },
    "a8f02048df2b41c396ad60400fde9134": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d14174dd4d494c9c9d7f83e1bc1e54f9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_082dae9e17754718a47db21e5482eebb",
      "value": "‚Äá499M/499M‚Äá[00:01&lt;00:00,‚Äá392MB/s]"
     }
    },
    "cc26941f2a4f455d9e3fff49a0d690f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "593fbc1b8eba49b7857d0d3a73329eb5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d8b4aeb193348a5891137a7c6e7df53": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e63aa202f7954802a6bdb3b87a177b01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d8858c31d964ca085a73b1c06b2801c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d14174dd4d494c9c9d7f83e1bc1e54f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "082dae9e17754718a47db21e5482eebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -qq datasets"
   ],
   "metadata": {
    "id": "uEkQQgj2QVPj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb7cac96-348c-4a9a-8931-8efee0872ae9",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:11:51.254570Z",
     "start_time": "2025-03-22T09:11:46.731966Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0h003HVeJtt7",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:12:48.558094Z",
     "start_time": "2025-03-22T09:12:48.548360Z"
    }
   },
   "source": [
    "# load data\n",
    "import os\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "train_conll= os.path.join(DATA_DIR, \"train.conll\")\n",
    "val_conll = os.path.join(DATA_DIR, \"val.conll\")\n",
    "test_conll = os.path.join(DATA_DIR, \"test.conll\")\n",
    "print(train_conll)\n",
    "print(val_conll)\n",
    "print(test_conll)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\DHCNTT\\HK6\\NER-VNLegalText-BERT\\data\\train.conll\n",
      "C:\\Users\\Admin\\DHCNTT\\HK6\\NER-VNLegalText-BERT\\data\\val.conll\n",
      "C:\\Users\\Admin\\DHCNTT\\HK6\\NER-VNLegalText-BERT\\data\\test.conll\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "def read_conll(file_path):\n",
    "    sentences = []\n",
    "    sentence_labels = []\n",
    "    unique_labels = set()  # To collect unique labels\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_sentence_tokens = []\n",
    "        current_sentence_labels = []\n",
    "\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace, including '\\n'\n",
    "\n",
    "            # If it's an empty line, sentence boundary detected\n",
    "            if not line:\n",
    "                if current_sentence_tokens:  # Check if there's a sentence to append\n",
    "                    sentences.append(' '.join(current_sentence_tokens))\n",
    "                    sentence_labels.append(' '.join(current_sentence_labels))\n",
    "                current_sentence_tokens = []  # Reset for the next sentence\n",
    "                current_sentence_labels = []  # Reset for the next sentence\n",
    "            else:\n",
    "                line_parts = line.split()  # Split line into token and label\n",
    "                current_sentence_tokens.append(line_parts[0])\n",
    "\n",
    "                if len(line_parts) >= 2:\n",
    "                    current_sentence_labels.append(line_parts[1])\n",
    "                    unique_labels.add(line_parts[1])  # Add label to the set of unique labels\n",
    "                else:\n",
    "                    current_sentence_labels.append('O')  # Default to 'O' if no label provided\n",
    "\n",
    "    # Append the last sentence if the file doesn't end with an empty line\n",
    "    if current_sentence_tokens:\n",
    "        sentences.append(' '.join(current_sentence_tokens))\n",
    "        sentence_labels.append(' '.join(current_sentence_labels))\n",
    "\n",
    "    print(f\"Unique labels found: {unique_labels}\")\n",
    "    return sentences, sentence_labels\n",
    "\n",
    "# Load the datasets\n",
    "test_sentences, test_labels = read_conll(test_conll)\n",
    "dev_sentences, dev_labels = read_conll(val_conll)\n",
    "train_sentences, train_labels = read_conll(train_conll)\n",
    "\n",
    "# Now, test_sentences, test_labels, dev_sentences, dev_labels, train_sentences, and train_labels are arrays of strings\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJYBvre1KXnM",
    "outputId": "d88a38dd-ae8c-4810-cd65-55f785ab9ed2",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:13:14.593555Z",
     "start_time": "2025-03-22T09:13:13.384785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: {'I-TTLT', 'B-TT', 'I-HP', 'B-PL', 'B-TTLT', 'B-Qƒê', 'I-PL', 'I-BL', 'B-HP', 'I-TT', 'I-NQ', 'B-BL', 'I-L', 'B-L', 'I-Qƒê', 'I-Nƒê', 'B-Nƒê', 'O', 'B-NQ'}\n",
      "Unique labels found: {'B-TT', 'I-HP', 'B-PL', 'I-PL', 'B-Qƒê', 'B-TTLT', 'I-BL', 'B-HP', 'I-TT', 'B-BL', 'I-NQ', 'I-L', 'B-L', 'I-Qƒê', 'I-Nƒê', 'B-Nƒê', 'O', 'B-NQ', 'I-TTLT'}\n",
      "Unique labels found: {'I-TTLT', 'B-TT', 'I-HP', 'B-PL', 'B-TTLT', 'B-Qƒê', 'I-PL', 'I-BL', 'B-HP', 'I-TT', 'I-NQ', 'B-BL', 'I-L', 'B-L', 'I-Qƒê', 'I-Nƒê', 'B-Nƒê', 'O', 'B-NQ'}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "test_sentences[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "GOu89ewfKbcN",
    "outputId": "9c673944-feb9-477e-f9c5-1475291672c5",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:13:19.799964Z",
     "start_time": "2025-03-22T09:13:19.781055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ngh·ªã ƒë·ªãnh ƒêi·ªÅu ch·ªânh thu nh·∫≠p th√°ng ƒë√£ ƒë√≥ng b·∫£o hi·ªÉm x√£ h·ªôi ƒë·ªëi v·ªõi ng∆∞·ªùi lao ƒë·ªông tham gia b·∫£o hi·ªÉm x√£ h·ªôi t·ª± nguy·ªán CƒÉn_c·ª© Lu·∫≠t T·ªï_ch·ª©c Ch√≠nh_ph·ªß ng√†y 25 th√°ng 12 nƒÉm 2001 .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "test_labels[1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Uf52SCP5Kgzn",
    "outputId": "fc992dc9-26d5-4368-f00b-599a91fda6ca",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:13:22.405595Z",
     "start_time": "2025-03-22T09:13:22.398600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O O O O O O O O O O O O O O O O O O O O O O O O O O O B-L I-L I-L I-L I-L I-L I-L I-L I-L O'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Step 1: Prepare the datasets from sentences and labels\n",
    "def prepare_dataset(sentences, labels):\n",
    "    return {'tokens': sentences, 'labels': labels}\n",
    "\n",
    "train_dataset = prepare_dataset(train_sentences, train_labels)\n",
    "dev_dataset = prepare_dataset(dev_sentences, dev_labels)\n",
    "test_dataset = prepare_dataset(test_sentences, test_labels)\n",
    "\n",
    "# Step 2: Convert strings of tokens and labels into arrays\n",
    "def process_string_to_array(dataset):\n",
    "    return {\n",
    "        'tokens': [sentence.split() for sentence in dataset['tokens']],\n",
    "        'labels': [label_seq.split() for label_seq in dataset['labels']]\n",
    "    }\n",
    "\n",
    "# Step 3: Process the dataset for token and label lists\n",
    "train_dataset = process_string_to_array(train_dataset)\n",
    "dev_dataset = process_string_to_array(dev_dataset)\n",
    "test_dataset = process_string_to_array(test_dataset)\n",
    "\n",
    "# Step 4: Convert processed datasets into Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "dev_dataset = Dataset.from_dict(dev_dataset)\n",
    "test_dataset = Dataset.from_dict(test_dataset)\n",
    "\n",
    "# Print the size of each dataset and a sample for verification\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(\"Train dataset sample:\", train_dataset[0])\n",
    "print(\"Dev dataset sample:\", dev_dataset[0])\n",
    "print(\"Test dataset sample:\", test_dataset[0])\n",
    "\n",
    "# Step 5: Define an Example class\n",
    "class Example:\n",
    "    def __init__(self, words, slot_labels, guid=None):\n",
    "        self.words = words\n",
    "        self.slot_labels = slot_labels\n",
    "        self.guid = guid\n",
    "\n",
    "# Step 6: Convert the dataset to Example objects\n",
    "def convert_to_examples(dataset):\n",
    "    return [\n",
    "        Example(words=tokens, slot_labels=labels, guid=i)\n",
    "        for i, (tokens, labels) in enumerate(zip(dataset['tokens'], dataset['labels']))\n",
    "    ]\n",
    "\n",
    "# Convert datasets into Example objects\n",
    "train_examples = convert_to_examples(train_dataset)\n",
    "dev_examples = convert_to_examples(dev_dataset)\n",
    "test_examples = convert_to_examples(test_dataset)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpwbDRI4bXY0",
    "outputId": "9aa96aac-793b-43f0-fd49-ec518d7becdb",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:13:37.825614Z",
     "start_time": "2025-03-22T09:13:24.992723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 34180\n",
      "Dev dataset size: 4272\n",
      "Test dataset size: 4273\n",
      "Train dataset sample: {'tokens': ['Trong', 'th·ªùi_gian', 't·ª´', 'khi', 'n·ªôp', 'h·ªì_s∆°', 'ƒëƒÉng_k√Ω_ch·ªß', 'ngu·ªìn', 'th·∫£i', 'CTNH', 'cho', 'ƒë·∫øn', 'khi', 'ƒë∆∞·ª£c', 'c·∫•p', 'S·ªï', 'ƒëƒÉng_k√Ω', ',', 'ch·ªß', 'ngu·ªìn', 'th·∫£i', 'CTNH', 'ƒë∆∞·ª£c', 'coi', 'l√†', 'ƒë√£', 'th·ª±c_hi·ªán', 'tr√°ch_nhi·ªám', 'ƒëƒÉng_k√Ω', 'v·ªÅ', 'vi·ªác', 'ph√°t_sinh', 'CTNH', 'v·ªõi', 'c∆°_quan', 'chuy√™n_m√¥n', 'v·ªÅ', 'b·∫£o_v·ªá', 'm√¥i_tr∆∞·ªùng', 'c·∫•p', 't·ªânh', 'theo', 'quy_ƒë·ªãnh', 't·∫°i', 'Kho·∫£n', '1', 'ƒêi·ªÅu', '70', 'Lu·∫≠t', 'B·∫£o_v·ªá', 'm√¥i_tr∆∞·ªùng', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-L', 'I-L', 'I-L', 'O']}\n",
      "Dev dataset sample: {'tokens': ['S·ª≠a_ƒë·ªïi', ',', 'b·ªï_sung', 'm·ªôt_s·ªë', 'ƒëi·ªÅu', 'c·ªßa', 'Ngh·ªã_ƒë·ªãnh', 's·ªë', '204/2004/Nƒê-CP', 'ng√†y', '14', 'th√°ng', '12', 'nƒÉm', '2004', 'v·ªÅ', 'ch·∫ø_ƒë·ªô', 'ti·ªÅn_l∆∞∆°ng', 'ƒë·ªëi_v·ªõi', 'c√°n_b·ªô', ',', 'c√¥ng_ch·ª©c', ',', 'vi√™n_ch·ª©c', 'v√†', 'l·ª±c_l∆∞·ª£ng', 'v≈©_trang', '(', 'sau', 'ƒë√¢y', 'vi·∫øt', 't·∫Øt', 'l√†', 'Ngh·ªã_ƒë·ªãnh', 's·ªë', '204/2004/Nƒê-CP', ')', 'nh∆∞', 'sau', ':', '1', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'B-Nƒê', 'I-Nƒê', 'I-Nƒê', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "Test dataset sample: {'tokens': ['C√°c', 'd·ª±_√°n', 'h√≥a_ch·∫•t', 'thu·ªôc', 'nh√≥m', 'C', 'ph·∫£i', 'x√¢y_d·ª±ng', 'K·∫ø_ho·∫°ch', 'theo', 'quy_ƒë·ªãnh', 't·∫°i', 'Th√¥ng_t∆∞', 's·ªë', '28/2010/TT-BCT', 'ng√†y', '28', 'th√°ng', '6', 'nƒÉm', '2010', 'c·ªßa', 'B·ªô', 'C√¥ng_Th∆∞∆°ng', 'quy_ƒë·ªãnh', 'c·ª•_th·ªÉ', 'm·ªôt_s·ªë', 'ƒëi·ªÅu', 'c·ªßa', 'Lu·∫≠t_H√≥a_ch·∫•t', 'v√†', 'Ngh·ªã_ƒë·ªãnh', 's·ªë', '108/2008/Nƒê-CP', 'ng√†y', '07', 'th√°ng', '10', 'nƒÉm', '2008', 'c·ªßa', 'Ch√≠nh_ph·ªß', 'quy_ƒë·ªãnh', 'chi_ti·∫øt', 'v√†', 'h∆∞·ªõng_d·∫´n', 'thi_h√†nh', 'm·ªôt_s·ªë', 'ƒëi·ªÅu', 'c·ªßa', 'Lu·∫≠t_H√≥a_ch·∫•t', 'ƒë√£', 'ƒë∆∞·ª£c', 'S·ªü', 'C√¥ng_Th∆∞∆°ng', 'ph√™_duy·ªát', 'tr∆∞·ªõc', 'ng√†y', 'Th√¥ng_t∆∞', 'n√†y', 'c√≥', 'hi·ªáu_l·ª±c', 'thi_h√†nh', 'th√¨', 'kh√¥ng', 'ph·∫£i', 'l√†m', 'h·ªì_s∆°', 'ƒë·ªÅ_ngh·ªã', 'B·ªô', 'C√¥ng_Th∆∞∆°ng', 'th·∫©m_ƒë·ªãnh', 'v√†', 'ph√™_duy·ªát', 'l·∫°i', 'K·∫ø_ho·∫°ch', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TT', 'I-TT', 'I-TT', 'I-TT', 'I-TT', 'I-TT', 'I-TT', 'I-TT', 'I-TT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-L', 'O', 'B-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'I-Nƒê', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os"
   ],
   "metadata": {
    "id": "KY8Z5RMYS0pU",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:13:43.619582Z",
     "start_time": "2025-03-22T09:13:43.603492Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_labels(file_path):\n",
    "    labels = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # B·ªè qua d√≤ng tr·ªëng\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:  # N·∫øu c√≥ label\n",
    "                    labels.add(parts[1])\n",
    "    return labels\n",
    "\n",
    "# G·ªôp labels t·ª´ t·∫•t c·∫£ c√°c file\n",
    "all_labels = set()\n",
    "for file_path in [train_conll, val_conll, test_conll]:\n",
    "    file_labels = extract_labels(file_path)\n",
    "    all_labels.update(file_labels)\n",
    "\n",
    "sorted_labels = sorted(list(all_labels))\n",
    "label_map = {label: i for i, label in enumerate(sorted_labels)}\n",
    "\n",
    "print(f\"\\n‚ú® Total labels: {len(label_map)}\")\n",
    "label_list= list(label_map.keys())\n",
    "print(\"Label Map:\", label_list)\n"
   ],
   "metadata": {
    "id": "gPwTywxOTC2_",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:16:01.668744Z",
     "start_time": "2025-03-22T09:16:00.973162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Total labels: 19\n",
      "Label Map: ['B-BL', 'B-HP', 'B-L', 'B-NQ', 'B-Nƒê', 'B-PL', 'B-Qƒê', 'B-TT', 'B-TTLT', 'I-BL', 'I-HP', 'I-L', 'I-NQ', 'I-Nƒê', 'I-PL', 'I-Qƒê', 'I-TT', 'I-TTLT', 'O']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "id": "93kM19XPSvix",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:16:17.578330Z",
     "start_time": "2025-03-22T09:16:17.564214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    max_seq_len,\n",
    "    tokenizer,\n",
    "    pad_label_id=-100,\n",
    "    cls_token_segment_id=0,\n",
    "    pad_token_segment_id=0,\n",
    "    sequence_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    # Get special tokens from the tokenizer\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # List to hold the converted features\n",
    "    features = []\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        # Log progress every 5000 examples\n",
    "        if example_index % 400 == 0:\n",
    "            logger.info(f\"Processing example {example_index} of {len(examples)}\")\n",
    "\n",
    "        # Tokenize each word and align its corresponding label\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "\n",
    "        for word, label in zip(example.words, example.slot_labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # If the word cannot be tokenized, use [UNK] token\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]\n",
    "\n",
    "            tokens.extend(word_tokens)\n",
    "\n",
    "            # Map string label to integer ID, apply pad_label_id for subword tokens\n",
    "            label_id = label_map[label]\n",
    "            label_ids.extend([label_id] + [pad_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Handle sequence truncation for [CLS] and [SEP] tokens\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:max_seq_len - special_tokens_count]\n",
    "            label_ids = label_ids[:max_seq_len - special_tokens_count]\n",
    "\n",
    "        # Add [SEP] token at the end of the sentence\n",
    "        tokens.append(sep_token)\n",
    "        label_ids.append(pad_label_id)\n",
    "        token_type_ids = [sequence_segment_id] * len(tokens)\n",
    "\n",
    "        # Add [CLS] token at the start of the sentence\n",
    "        tokens = [cls_token] + tokens\n",
    "        label_ids = [pad_label_id] + label_ids\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "\n",
    "        # Convert tokens to input IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Create attention masks (1 for real tokens, 0 for padding tokens)\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "\n",
    "        # Pad sequences to the maximum sequence length\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids += [pad_token_id] * padding_length\n",
    "        attention_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "        token_type_ids += [pad_token_segment_id] * padding_length\n",
    "        label_ids += [pad_label_id] * padding_length\n",
    "\n",
    "        # Create InputFeatures object and append it to the list of features\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                slot_labels_ids=label_ids,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return features\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "import json"
   ],
   "metadata": {
    "id": "j1gDFp4WTtg2",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:16:27.100680Z",
     "start_time": "2025-03-22T09:16:27.093529Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, slot_labels_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.slot_labels_ids = slot_labels_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ],
   "metadata": {
    "id": "_1BFV0caTUT6",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:16:32.831848Z",
     "start_time": "2025-03-22T09:16:32.822156Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "tokenizer.cls_token, tokenizer.sep_token, tokenizer.unk_token, tokenizer.pad_token_id"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3jxMCeOLr8z",
    "outputId": "dd3aa9fe-6157-4190-a672-9d118f97cc0b",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:17:39.226599Z",
     "start_time": "2025-03-22T09:17:38.168502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>', '<unk>', 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "\n",
    "# Set the maximum sequence length\n",
    "max_seq_len = 128  # You can adjust this based on your model/input\n",
    "\n",
    "# Convert examples to features\n",
    "train_features = convert_examples_to_features(train_examples, max_seq_len, tokenizer)\n",
    "dev_features = convert_examples_to_features(dev_examples, max_seq_len, tokenizer)\n",
    "test_features = convert_examples_to_features(test_examples, max_seq_len, tokenizer)\n"
   ],
   "metadata": {
    "id": "8L7QmU9OSkQ_",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:19:39.536Z",
     "start_time": "2025-03-22T09:18:20.784021Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a Dataset class to wrap the tokenized features for training\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(feature.input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(feature.attention_mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(feature.token_type_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(feature.slot_labels_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Convert tokenized features into PyTorch datasets\n",
    "train_dataset = NERDataset(train_features)\n",
    "dev_dataset = NERDataset(dev_features)\n",
    "test_dataset = NERDataset(test_features)\n"
   ],
   "metadata": {
    "id": "wIpgQ1koT5um",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:20:00.311452Z",
     "start_time": "2025-03-22T09:20:00.305009Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dbw028u8Na6Z",
    "outputId": "13d27053-fbb6-4426-c14a-0daa70b3ed7e",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:20:05.275386Z",
     "start_time": "2025-03-22T09:20:04.156031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  2393,  1657,  3553,  1376,  2023,    46,   118,  1215, 42859,\n",
       "           326,  1376,  2023,  4958,   449,  3592,   295,  1376,  2023,    27,\n",
       "           642,  1368,  1376,  2023,  9085,  1215,    29,  8188,  5543,  4236,\n",
       "          3602,   649,   862,  2590,  1215,   330,  3849, 10809,  1215,   611,\n",
       "          1376,  2023,  6248,   295,  5521,  1376,  2023,  9085,   282,  3553,\n",
       "          1376,  3070,  2469,   118, 12464, 28812, 14310,  4236,  3602,  1376,\n",
       "          3070,  9470,   282,   449,  3592,  4236,  3602,  8188,  7487,  1376,\n",
       "          2023,  2469,   438,   740,  1376,  3070,  8210,   642,   208,  1376,\n",
       "          2023, 15722,  4236,  3602,   649,   862,  2590,  1215,   330,  3849,\n",
       "         10809,  2156,  1855,  1376,  2023,  6248,   295,  5521,  1376,  2023,\n",
       "          9085,   282,  3553,  1376,  3070,  2469,   118, 12464, 28812,  4236,\n",
       "          3602,  8188,  7487,  1376,  2023,  2469,   438,  1029,   118,   784,\n",
       "          5269,  4236,  3602, 17682,  3553,  1376,  2023,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([-100,   18, -100,   18, -100, -100, -100, -100, -100, -100,   18, -100,\n",
       "         -100, -100,   18, -100,   18, -100, -100, -100, -100,   18, -100, -100,\n",
       "         -100, -100, -100, -100, -100,   18, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100,   18, -100, -100, -100, -100,\n",
       "         -100,   18, -100, -100, -100, -100,   18, -100,   18,   18, -100, -100,\n",
       "         -100, -100, -100,   18, -100,   18, -100, -100, -100, -100, -100, -100,\n",
       "         -100,   18, -100, -100, -100, -100,   18, -100, -100, -100,   18, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100,   18,   18, -100, -100, -100,\n",
       "           18, -100, -100, -100, -100, -100,   18, -100, -100, -100, -100,   18,\n",
       "         -100,   18, -100, -100, -100, -100, -100, -100, -100,   18, -100,   18,\n",
       "         -100,   18, -100, -100,   18, -100, -100, -100])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import RobertaForTokenClassification\n",
    "\n",
    "# Define the number of unique labels (ensure this matches your dataset's label set)\n",
    "num_labels = len(label_list)  # e.g., the number of unique labels such as O, B-ORG, etc.\n",
    "\n",
    "# Load the RoBERTa model for token classification\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=num_labels)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "eb3ae4371d214130b82ecd897e5c918d",
      "1e713a4ba99d436d9efb0011bb74bf8c",
      "5b69a6e548724954855ef5cefe8cbd45",
      "a8f02048df2b41c396ad60400fde9134",
      "cc26941f2a4f455d9e3fff49a0d690f7",
      "593fbc1b8eba49b7857d0d3a73329eb5",
      "0d8b4aeb193348a5891137a7c6e7df53",
      "e63aa202f7954802a6bdb3b87a177b01",
      "6d8858c31d964ca085a73b1c06b2801c",
      "d14174dd4d494c9c9d7f83e1bc1e54f9",
      "082dae9e17754718a47db21e5482eebb"
     ]
    },
    "id": "4giDkLYOUB57",
    "outputId": "69636a80-5b44-48ee-8021-06e688de857e",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:20:17.071970Z",
     "start_time": "2025-03-22T09:20:16.174717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T09:23:41.823368Z",
     "start_time": "2025-03-22T09:23:41.816246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "output_dir = './results'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"created folder: {output_dir}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created folder: ./results\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,           # output directory to save model checkpoints and results\n",
    "    evaluation_strategy=\"epoch\",      # evaluation is done at the end of every epoch\n",
    "    per_device_train_batch_size=16,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
    "    num_train_epochs=3,               # number of epochs to train the model\n",
    "    weight_decay=0.01,                # strength of weight decay\n",
    "    logging_dir='./logs',             # directory for storing logs\n",
    "    logging_steps=10,                 # log every 10 steps\n",
    "    save_steps=500,                   # save model checkpoint every 500 steps\n",
    "    save_total_limit=2,               # limit the number of total checkpoints to save\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wD9Coch-UECW",
    "outputId": "74724e71-536e-4131-8a8a-8fba5ba76a58",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:24:26.983280Z",
     "start_time": "2025-03-22T09:24:26.903519Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install seqeval"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CmjNJmGW8Qe",
    "outputId": "2c8d0610-8765-409d-ca4d-d1250cbc617d",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:24:32.720796Z",
     "start_time": "2025-03-22T09:24:29.947994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\admin\\miniconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "source": [
    "label_list"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C3PSdHdY0zE",
    "outputId": "472e7d93-8014-44c1-f831-9eff2af8e42a",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:24:36.379635Z",
     "start_time": "2025-03-22T09:24:36.372658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-BL',\n",
       " 'B-HP',\n",
       " 'B-L',\n",
       " 'B-NQ',\n",
       " 'B-Nƒê',\n",
       " 'B-PL',\n",
       " 'B-Qƒê',\n",
       " 'B-TT',\n",
       " 'B-TTLT',\n",
       " 'I-BL',\n",
       " 'I-HP',\n",
       " 'I-L',\n",
       " 'I-NQ',\n",
       " 'I-Nƒê',\n",
       " 'I-PL',\n",
       " 'I-Qƒê',\n",
       " 'I-TT',\n",
       " 'I-TTLT',\n",
       " 'O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import EvalPrediction\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions = p.predictions.argmax(axis=2)  # Get predicted label indices\n",
    "    labels = p.label_ids  # True label IDs\n",
    "\n",
    "    # Debugging: Print shapes of predictions and labels\n",
    "    print(f\"Shape of predictions: {predictions.shape}\")\n",
    "    print(f\"Shape of labels: {labels.shape}\")\n",
    "\n",
    "    # Debugging: Log first few predictions and labels for inspection\n",
    "    print(f\"First few predictions: {predictions[:2]}\")\n",
    "    print(f\"First few labels: {labels[:2]}\")\n",
    "\n",
    "    pred_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Iterate through predictions and labels\n",
    "    for i, (pred_seq, true_seq) in enumerate(zip(predictions, labels)):\n",
    "        pred_label_seq = []\n",
    "        true_label_seq = []\n",
    "\n",
    "        # Iterate through each token in the sequence\n",
    "        for pred_idx, true_idx in zip(pred_seq, true_seq):\n",
    "            if true_idx == -100:\n",
    "                # Debugging: Log any padding tokens encountered\n",
    "                # print(f\"Padding token encountered at position {i}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the indices are within the valid range\n",
    "            if pred_idx < len(label_list) and true_idx < len(label_list):\n",
    "                pred_label_seq.append(label_list[pred_idx])\n",
    "                true_label_seq.append(label_list[true_idx])\n",
    "            else:\n",
    "                # Debugging: Log when out-of-bound indices are encountered\n",
    "                print(f\"Index out of range: pred_idx={pred_idx}, true_idx={true_idx} at position {i}\")\n",
    "\n",
    "        pred_labels.append(pred_label_seq)\n",
    "        true_labels.append(true_label_seq)\n",
    "\n",
    "    # Debugging: Log final processed predictions and labels\n",
    "    print(f\"Processed pred_labels: {pred_labels[:2]}\")\n",
    "    print(f\"Processed true_labels: {true_labels[:2]}\")\n",
    "\n",
    "    # Compute token-level F1, Precision, and Recall\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    # Trong 10 l·∫ßn d·ª± ƒëo√°n nh√£n PER: th√¨ ch√∫ng ta ƒëo√°n ƒë√∫ng 6 l·∫ßn -> 6/10 = 60%\n",
    "\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    # Trong 8 nh√£n PER th·∫≠t: th√¨ ch√∫ng ta ƒëo√°n ƒë√∫ng 6 l·∫ßn -> 6/8 = 75%\n",
    "\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "    # Debugging: Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ],
   "metadata": {
    "id": "L3gD8zJQW4j_",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:24:52.931022Z",
     "start_time": "2025-03-22T09:24:52.921959Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction\n"
   ],
   "metadata": {
    "id": "ot0X4_cLb1vT",
    "ExecuteTime": {
     "end_time": "2025-03-22T09:24:59.684592Z",
     "start_time": "2025-03-22T09:24:59.672166Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize the Trainer with the modified compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics  # Updated function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "id": "jxgd98KnUJzP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "7317bd74-6896-4f61-a020-ececcbe2e60e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [945/945 06:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.252638</td>\n",
       "      <td>0.789760</td>\n",
       "      <td>0.840457</td>\n",
       "      <td>0.814320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.131611</td>\n",
       "      <td>0.885309</td>\n",
       "      <td>0.899170</td>\n",
       "      <td>0.892186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.127506</td>\n",
       "      <td>0.885836</td>\n",
       "      <td>0.913574</td>\n",
       "      <td>0.899491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of predictions: (2000, 128)\n",
      "Shape of labels: (2000, 128)\n",
      "First few predictions: [[ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 16  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6 16 17  6  6 17 17 17 17 17 17 17  6  6  6  6\n",
      "   6  6  6 16 16 17 17 17 17  6 16 17 17 17 17 17 17 17 17 17  6 17 17  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6 10  5  5  6]\n",
      " [ 6  6  6  6  6  6  6  6  6  6  6  6  6  7  7  6  6 14  6  6 19  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 16\n",
      "  16 17 17 17 17 17 17 17 17  6 16 17 17  6 17 17 17  6 16 16  6 17 17 17\n",
      "  17 17  6  6  6  6  6  6  6  6 16 16 17 17 17 17 17 17 17 17  6 16 17 17\n",
      "  17  6 17  6 17  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  7  7  6  6  6  6  6]]\n",
      "First few labels: [[-100    6 -100 -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100    6    6 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    0 -100 -100 -100 -100 -100   18 -100 -100\n",
      "  -100 -100   18 -100 -100 -100 -100 -100   18   18 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100   18 -100 -100 -100 -100 -100 -100\n",
      "  -100    6    6    6 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100 -100 -100    6\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100    6 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6 -100 -100   10 -100\n",
      "  -100 -100]\n",
      " [-100    6    6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100    7\n",
      "  -100    6    6   14 -100    6   19    6 -100 -100 -100 -100    6    6\n",
      "  -100    2 -100 -100 -100 -100 -100 -100 -100    9 -100    9 -100 -100\n",
      "    16 -100 -100 -100 -100   17   17 -100 -100 -100   17 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6   16 -100 -100 -100\n",
      "  -100 -100 -100 -100    6    6 -100    6 -100 -100 -100 -100   16 -100\n",
      "  -100 -100 -100   17 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100    6    6 -100 -100    6 -100 -100 -100    6    6 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    7 -100    6    6    6\n",
      "  -100 -100]]\n",
      "Processed pred_labels: [['O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Processed true_labels: [['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'B-JOB', 'I-JOB', 'I-JOB', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Classification Report:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                AGE       0.88      0.97      0.92       308\n",
      "               DATE       0.94      0.99      0.96       993\n",
      "             GENDER       0.85      0.94      0.90       245\n",
      "                JOB       0.00      0.00      0.00       112\n",
      "           LOCATION       0.70      0.88      0.78      2295\n",
      "               NAME       0.96      0.73      0.83       169\n",
      "       ORGANIZATION       0.54      0.33      0.41       500\n",
      "         PATIENT_ID       0.93      0.98      0.96      1067\n",
      "SYMPTOM_AND_DISEASE       0.71      0.70      0.70       619\n",
      "     TRANSPORTATION       0.86      0.85      0.85        79\n",
      "\n",
      "          micro avg       0.79      0.84      0.81      6387\n",
      "          macro avg       0.74      0.74      0.73      6387\n",
      "       weighted avg       0.78      0.84      0.80      6387\n",
      "\n",
      "Shape of predictions: (2000, 128)\n",
      "Shape of labels: (2000, 128)\n",
      "First few predictions: [[ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  0 18  6 18 18 18 18 18 18 18 18 18 18 18 18\n",
      "  18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6 10  5  5  6]\n",
      " [ 6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  6  6 14  6  6 19  6  6  6\n",
      "   6  6  6  6  6 14  6  6  6  6  6  6  6  6  6  6  6 17  6  6  6  6  6 16\n",
      "  17 17 17 17 17 17 17 17 17 17 16 17 17 17 17 17 17  6 16 17 17 17 17 17\n",
      "  17 17  6  6  6  6  6  6  6  6 16 17 17 17 17 17 17 17 17 17 17 17 17 17\n",
      "  17 17 17  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   7  7  7  6  6  6  6  6]]\n",
      "First few labels: [[-100    6 -100 -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100    6    6 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    0 -100 -100 -100 -100 -100   18 -100 -100\n",
      "  -100 -100   18 -100 -100 -100 -100 -100   18   18 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100   18 -100 -100 -100 -100 -100 -100\n",
      "  -100    6    6    6 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100 -100 -100    6\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100    6 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6 -100 -100   10 -100\n",
      "  -100 -100]\n",
      " [-100    6    6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100    7\n",
      "  -100    6    6   14 -100    6   19    6 -100 -100 -100 -100    6    6\n",
      "  -100    2 -100 -100 -100 -100 -100 -100 -100    9 -100    9 -100 -100\n",
      "    16 -100 -100 -100 -100   17   17 -100 -100 -100   17 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6   16 -100 -100 -100\n",
      "  -100 -100 -100 -100    6    6 -100    6 -100 -100 -100 -100   16 -100\n",
      "  -100 -100 -100   17 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100    6    6 -100 -100    6 -100 -100 -100    6    6 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    7 -100    6    6    6\n",
      "  -100 -100]]\n",
      "Processed pred_labels: [['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'B-GENDER', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Processed true_labels: [['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'B-JOB', 'I-JOB', 'I-JOB', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                AGE       0.93      0.97      0.95       308\n",
      "               DATE       0.97      0.99      0.98       993\n",
      "             GENDER       0.84      0.95      0.89       245\n",
      "                JOB       0.79      0.24      0.37       112\n",
      "           LOCATION       0.89      0.89      0.89      2295\n",
      "               NAME       0.91      0.80      0.85       169\n",
      "       ORGANIZATION       0.71      0.79      0.75       500\n",
      "         PATIENT_ID       0.95      0.99      0.97      1067\n",
      "SYMPTOM_AND_DISEASE       0.76      0.78      0.77       619\n",
      "     TRANSPORTATION       0.91      0.97      0.94        79\n",
      "\n",
      "          micro avg       0.89      0.90      0.89      6387\n",
      "          macro avg       0.87      0.84      0.84      6387\n",
      "       weighted avg       0.89      0.90      0.89      6387\n",
      "\n",
      "Shape of predictions: (2000, 128)\n",
      "Shape of labels: (2000, 128)\n",
      "First few predictions: [[ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  0 18  6 18 18 18 18 18 18 18 18 18 18 18 18\n",
      "  18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  6  6  6 10  5  5  6]\n",
      " [ 6  6  6  6  6  6  6  6  6  6  6  6  6  7  7  6  6 14 14  6 19  6  6  6\n",
      "   6  6  6  6  6  2  2  6  6  6  2  6  6  6  6  6  6  6  6  6  6  6  6  0\n",
      "  16 17 17 17 17 17 17 17 17 17 16 17 17 17 17 17 17  6 16 16 17 17 17 17\n",
      "  17 17  6  6  6  6  6  6  6  6 16 16 17 17 17 17 17 17 17 17 17 16 17 17\n",
      "  17 17 17 17 17  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "   6  7  7  6  6  6  6  6]]\n",
      "First few labels: [[-100    6 -100 -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100    6    6 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    0 -100 -100 -100 -100 -100   18 -100 -100\n",
      "  -100 -100   18 -100 -100 -100 -100 -100   18   18 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100   18 -100 -100 -100 -100 -100 -100\n",
      "  -100    6    6    6 -100 -100 -100 -100    6 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100 -100 -100    6\n",
      "  -100 -100 -100 -100 -100    6 -100 -100 -100 -100 -100    6 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6 -100 -100   10 -100\n",
      "  -100 -100]\n",
      " [-100    6    6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100    7\n",
      "  -100    6    6   14 -100    6   19    6 -100 -100 -100 -100    6    6\n",
      "  -100    2 -100 -100 -100 -100 -100 -100 -100    9 -100    9 -100 -100\n",
      "    16 -100 -100 -100 -100   17   17 -100 -100 -100   17 -100 -100 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    6   16 -100 -100 -100\n",
      "  -100 -100 -100 -100    6    6 -100    6 -100 -100 -100 -100   16 -100\n",
      "  -100 -100 -100   17 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "  -100 -100 -100    6    6 -100 -100    6 -100 -100 -100    6    6 -100\n",
      "  -100 -100 -100 -100 -100 -100 -100 -100 -100    7 -100    6    6    6\n",
      "  -100 -100]]\n",
      "Processed pred_labels: [['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'B-JOB', 'O', 'O', 'O', 'B-ORGANIZATION', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Processed true_labels: [['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE'], ['O', 'O', 'B-PATIENT_ID', 'O', 'O', 'B-GENDER', 'O', 'B-AGE', 'O', 'O', 'O', 'B-JOB', 'I-JOB', 'I-JOB', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'B-PATIENT_ID', 'O', 'O', 'O']]\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                AGE       0.94      0.97      0.95       308\n",
      "               DATE       0.97      0.99      0.98       993\n",
      "             GENDER       0.85      0.96      0.90       245\n",
      "                JOB       0.56      0.42      0.48       112\n",
      "           LOCATION       0.88      0.92      0.90      2295\n",
      "               NAME       0.93      0.85      0.89       169\n",
      "       ORGANIZATION       0.74      0.77      0.76       500\n",
      "         PATIENT_ID       0.98      0.99      0.99      1067\n",
      "SYMPTOM_AND_DISEASE       0.75      0.80      0.78       619\n",
      "     TRANSPORTATION       0.93      1.00      0.96        79\n",
      "\n",
      "          micro avg       0.89      0.91      0.90      6387\n",
      "          macro avg       0.85      0.87      0.86      6387\n",
      "       weighted avg       0.89      0.91      0.90      6387\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=945, training_loss=0.1582743813909551, metrics={'train_runtime': 380.1462, 'train_samples_per_second': 39.672, 'train_steps_per_second': 2.486, 'total_flos': 985314418007040.0, 'train_loss': 0.1582743813909551, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  }
 ]
}
